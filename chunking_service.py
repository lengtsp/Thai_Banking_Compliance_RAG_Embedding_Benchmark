import json
import httpx
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config import OLLAMA_BASE_URL, LLM_MODEL, RECURSIVE_CHUNK_SIZE, RECURSIVE_CHUNK_OVERLAP, llm_options


def create_recursive_chunks(pages: list[dict]) -> list[dict]:
    """
    Create recursive character text chunks from OCR pages.
    Each page is chunked independently to preserve page-level boundaries.
    Returns list of {page_number, chunk_index, chunk_text, chunk_size}.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=RECURSIVE_CHUNK_SIZE,
        chunk_overlap=RECURSIVE_CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", ".", " ", ""],
    )

    all_chunks = []
    for page in pages:
        page_num = page["page_number"]
        text = page["ocr_text"] or ""
        if not text.strip():
            continue

        chunks = splitter.split_text(text)
        for idx, chunk in enumerate(chunks):
            all_chunks.append({
                "page_number": page_num,
                "chunk_index": idx,
                "chunk_text": chunk,
                "chunk_size": len(chunk),
            })

    return all_chunks


def create_agentic_chunks(pages: list[dict]) -> list[dict]:
    """
    Use LLM to create semantic/agentic chunks from OCR pages.
    Each page is sent to the LLM to break into meaningful semantic sections.
    Returns list of {page_number, chunk_index, chunk_text, chunk_title, chunk_size}.
    """
    all_chunks = []

    for page in pages:
        page_num = page["page_number"]
        text = page["ocr_text"] or ""
        if not text.strip():
            continue

        prompt = f"""à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™à¸œà¸¹à¹‰à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¹ƒà¸™à¸à¸²à¸£à¹à¸šà¹ˆà¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸­à¸­à¸à¹€à¸›à¹‡à¸™à¸ªà¹ˆà¸§à¸™à¸¢à¹ˆà¸­à¸¢à¹€à¸Šà¸´à¸‡à¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢ (semantic chunking)

à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰ à¸‹à¸¶à¹ˆà¸‡à¹€à¸›à¹‡à¸™à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ {page_num} à¸‚à¸­à¸‡à¹€à¸­à¸à¸ªà¸²à¸£:

---
{text}
---

à¸à¸£à¸¸à¸“à¸²à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸™à¸µà¹‰à¸­à¸­à¸à¹€à¸›à¹‡à¸™ chunks à¹€à¸Šà¸´à¸‡à¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢ à¹‚à¸”à¸¢à¹à¸•à¹ˆà¸¥à¸° chunk à¸„à¸§à¸£:
1. à¸¡à¸µà¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸—à¸µà¹ˆà¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¹ƒà¸™à¸•à¸±à¸§à¹€à¸­à¸‡ (self-contained)
2. à¸¡à¸µà¸«à¸±à¸§à¸‚à¹‰à¸­à¸«à¸£à¸·à¸­à¸›à¸£à¸°à¹€à¸”à¹‡à¸™à¸«à¸¥à¸±à¸à¹€à¸”à¸µà¸¢à¸§
3. à¸¡à¸µà¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 1500 à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£

à¸•à¸­à¸šà¹€à¸›à¹‡à¸™ JSON array à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¹‚à¸”à¸¢à¹à¸•à¹ˆà¸¥à¸° element à¸¡à¸µ:
- "title": à¸«à¸±à¸§à¸‚à¹‰à¸­à¸ªà¸±à¹‰à¸™à¹† à¸‚à¸­à¸‡ chunk (à¸ à¸²à¸©à¸²à¹„à¸—à¸¢)
- "text": à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸‚à¸­à¸‡ chunk

à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸£à¸¹à¸›à¹à¸šà¸š:
[
  {{"title": "à¸«à¸±à¸§à¸‚à¹‰à¸­ 1", "text": "à¹€à¸™à¸·à¹‰à¸­à¸«à¸²..."}},
  {{"title": "à¸«à¸±à¸§à¸‚à¹‰à¸­ 2", "text": "à¹€à¸™à¸·à¹‰à¸­à¸«à¸²..."}}
]

à¸•à¸­à¸šà¹€à¸›à¹‡à¸™ JSON array à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸«à¹‰à¸²à¸¡à¸¡à¸µ text à¸­à¸·à¹ˆà¸™à¸™à¸­à¸à¹€à¸«à¸™à¸·à¸­à¸ˆà¸²à¸ JSON"""

        try:
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“ [Agentic Chunk Prompt â€” Page {page_num}] â†’ {LLM_MODEL}")
            print(prompt)
            print(f"{'â”€'*60}\n")

            payload = {
                "model": LLM_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": llm_options(),
            }

            with httpx.Client(timeout=600.0) as client:
                response = client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json=payload,
                )
                response.raise_for_status()
                result = response.json()
                llm_response = result.get("response", "").strip()

            # Parse JSON from LLM response
            # Try to extract JSON array from the response
            json_start = llm_response.find("[")
            json_end = llm_response.rfind("]") + 1
            if json_start >= 0 and json_end > json_start:
                json_str = llm_response[json_start:json_end]
                chunks_data = json.loads(json_str)
            else:
                # Fallback: treat entire page as one chunk
                chunks_data = [{"title": f"à¸«à¸™à¹‰à¸² {page_num}", "text": text}]

            for idx, chunk_data in enumerate(chunks_data):
                chunk_text = chunk_data.get("text", "")
                all_chunks.append({
                    "page_number": page_num,
                    "chunk_index": idx,
                    "chunk_text": chunk_text,
                    "chunk_title": chunk_data.get("title", ""),
                    "chunk_size": len(chunk_text),
                })

            print(f"  ğŸ§  Agentic chunking page {page_num}: {len(chunks_data)} chunks")

        except Exception as e:
            print(f"  âš ï¸ Agentic chunking failed for page {page_num}: {e}")
            # Fallback: treat entire page as one chunk
            all_chunks.append({
                "page_number": page_num,
                "chunk_index": 0,
                "chunk_text": text,
                "chunk_title": f"à¸«à¸™à¹‰à¸² {page_num} (fallback)",
                "chunk_size": len(text),
            })

    return all_chunks
