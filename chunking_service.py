import json
import httpx
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config import OLLAMA_BASE_URL, LLM_MODEL, RECURSIVE_CHUNK_SIZE, RECURSIVE_CHUNK_OVERLAP, llm_options


def _sanitize_json_str(s: str) -> str:
    """Escape raw control characters that appear inside JSON string values.

    LLMs sometimes emit literal newlines/tabs inside JSON strings instead of
    the escaped forms (\\n, \\t), which causes json.loads() to raise
    'Invalid control character'.  This function fixes that by walking the
    JSON character-by-character and replacing bare control chars with their
    proper JSON escape sequences only when inside a string literal.
    """
    result = []
    in_string = False
    escape_next = False
    for ch in s:
        if escape_next:
            result.append(ch)
            escape_next = False
        elif ch == "\\" and in_string:
            result.append(ch)
            escape_next = True
        elif ch == '"':
            in_string = not in_string
            result.append(ch)
        elif in_string and ord(ch) < 32:
            # bare control character inside a JSON string ‚Üí escape it
            if ch == "\n":
                result.append("\\n")
            elif ch == "\r":
                result.append("\\r")
            elif ch == "\t":
                result.append("\\t")
            else:
                result.append(f"\\u{ord(ch):04x}")
        else:
            result.append(ch)
    return "".join(result)


def _json_loads_safe(s: str):
    """Parse JSON, retrying with control-character sanitisation on failure."""
    try:
        return json.loads(s)
    except json.JSONDecodeError:
        return json.loads(_sanitize_json_str(s))


def create_recursive_chunks(pages: list[dict]) -> list[dict]:
    """
    Create recursive character text chunks from OCR pages.
    Each page is chunked independently to preserve page-level boundaries.
    Returns list of {page_number, chunk_index, chunk_text, chunk_size}.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=RECURSIVE_CHUNK_SIZE,
        chunk_overlap=RECURSIVE_CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", "„ÄÇ", ".", " ", ""],
    )

    all_chunks = []
    for page in pages:
        page_num = page["page_number"]
        text = page["ocr_text"] or ""
        if not text.strip():
            continue

        chunks = splitter.split_text(text)
        for idx, chunk in enumerate(chunks):
            all_chunks.append({
                "page_number": page_num,
                "chunk_index": idx,
                "chunk_text": chunk,
                "chunk_size": len(chunk),
            })

    return all_chunks


def create_agentic_chunks(pages: list[dict]) -> list[dict]:
    """
    Use LLM to create semantic/agentic chunks from OCR pages.
    Each page is sent to the LLM to break into meaningful semantic sections.
    Returns list of {page_number, chunk_index, chunk_text, chunk_title, chunk_size}.
    """
    all_chunks = []

    for page in pages:
        page_num = page["page_number"]
        text = page["ocr_text"] or ""
        if not text.strip():
            continue

        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (semantic chunking)

‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page_num} ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:

---
{text}
---

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô chunks ‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÇ‡∏î‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk ‡∏Ñ‡∏ß‡∏£:
1. ‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á (self-contained)
2. ‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
3. ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞ element ‡∏°‡∏µ:
- "title": ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏Ç‡∏≠‡∏á chunk (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
- "text": ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á chunk

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
[
  {{"title": "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ 1", "text": "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤..."}},
  {{"title": "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ 2", "text": "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤..."}}
]

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ text ‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å JSON"""

        try:
            print(f"\n{'‚îÄ'*60}")
            print(f"üìù [Agentic Chunk Prompt ‚Äî Page {page_num}] ‚Üí {LLM_MODEL}")
            print(prompt)
            print(f"{'‚îÄ'*60}\n")

            payload = {
                "model": LLM_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": llm_options(),
            }

            with httpx.Client(timeout=600.0) as client:
                response = client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json=payload,
                )
                response.raise_for_status()
                result = response.json()
                llm_response = result.get("response", "").strip()

            # Parse JSON from LLM response
            # Try to extract JSON array from the response
            json_start = llm_response.find("[")
            json_end = llm_response.rfind("]") + 1
            if json_start >= 0 and json_end > json_start:
                json_str = llm_response[json_start:json_end]
                chunks_data = _json_loads_safe(json_str)
            else:
                # Fallback: treat entire page as one chunk
                chunks_data = [{"title": f"‡∏´‡∏ô‡πâ‡∏≤ {page_num}", "text": text}]

            for idx, chunk_data in enumerate(chunks_data):
                chunk_text = chunk_data.get("text", "")
                all_chunks.append({
                    "page_number": page_num,
                    "chunk_index": idx,
                    "chunk_text": chunk_text,
                    "chunk_title": chunk_data.get("title", ""),
                    "chunk_size": len(chunk_text),
                })

            print(f"  üß† Agentic chunking page {page_num}: {len(chunks_data)} chunks")

        except Exception as e:
            print(f"  ‚ö†Ô∏è Agentic chunking failed for page {page_num}: {e}")
            # Fallback: treat entire page as one chunk
            all_chunks.append({
                "page_number": page_num,
                "chunk_index": 0,
                "chunk_text": text,
                "chunk_title": f"‡∏´‡∏ô‡πâ‡∏≤ {page_num} (fallback)",
                "chunk_size": len(text),
            })

    return all_chunks
