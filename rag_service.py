import numpy as np
import httpx
from config import (
    OLLAMA_BASE_URL, LLM_MODEL, RAG_TOP_K,
    EMBEDDING_MODELS,
    llm_options,
)
from embedding_service import get_embedding, retrieve_top_k

# Quick lookup: model key ‚Üí ollama model name
_MODEL_OLLAMA = {key: model for key, model, _ in EMBEDDING_MODELS}
_MODEL_LABEL  = {key: label for key, _, label in EMBEDDING_MODELS}


def generate_answer(question: str, context_chunks: list[dict]) -> dict:
    """Send question + retrieved context to LLM for answer generation.
    Returns {"answer": str, "prompt": str}.
    """
    context_text = "\n\n---\n\n".join([
        f"[Chunk {i+1}] (similarity: {c.get('similarity', 0):.4f}):\n{c['chunk_text']}"
        for i, c in enumerate(context_chunks)
    ])

    prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏Ç‡∏≠‡∏á‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢

‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

{context_text}

---

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤ ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏¢‡∏∑‡∏î‡∏¢‡∏≤‡∏ß ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"""

    print(f"\n{'‚îÄ'*60}")
    print(f"üìù [RAG Prompt] ‚Üí {LLM_MODEL}")
    print(prompt)
    print(f"{'‚îÄ'*60}\n")

    payload = {
        "model": LLM_MODEL,
        "prompt": prompt,
        "stream": False,
        "options": llm_options(),
    }

    with httpx.Client(timeout=600.0) as client:
        response = client.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json=payload,
        )
        response.raise_for_status()
        result = response.json()
        answer = result.get("response", "").strip()
    print(f"üí¨ [RAG Answer preview]: {answer[:200]}\n")
    return {"answer": answer, "prompt": prompt}


def run_rag_pipeline(
    questions: list[dict],
    embeddings_by_model: dict,   # {model_key: [{"chunk_text": str, "embedding": np.ndarray, ...}]}
    top_k: int = None,
) -> list[dict]:
    """
    Run full RAG pipeline for all questions across all embedding models.
    Per question: for each model ‚Üí retrieve top-k ‚Üí generate answer.

    Returns list of:
    {
        "question_number": int,
        "question_text": str,
        "golden_answer": str,
        "results_by_model": {
            model_key: {
                "retrieved_chunks": [...],
                "llm_answer": str,
                "llm_prompt": str,
            }
        }
    }
    """
    if top_k is None:
        top_k = RAG_TOP_K

    model_keys = [key for key, _, _ in EMBEDDING_MODELS if key in embeddings_by_model]
    all_results = []

    for q in questions:
        q_num  = q["question_number"]
        q_text = q["question_text"]
        print(f"\n{'='*60}")
        print(f"  üìù ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà {q_num}: {q_text[:80]}...")
        print(f"{'='*60}")

        results_by_model = {}
        for step, key in enumerate(model_keys, 1):
            label       = _MODEL_LABEL.get(key, key)
            ollama_model = _MODEL_OLLAMA.get(key, key)
            embeddings   = embeddings_by_model[key]

            print(f"\n  [Step {step}A] {label} ‚Äî Retrieving top-{top_k} chunks...")
            query_emb  = get_embedding(q_text, ollama_model)
            top_chunks = retrieve_top_k(query_emb, embeddings, top_k)
            top_sim    = top_chunks[0]["similarity"] if top_chunks else 0
            print(f"           Retrieved {len(top_chunks)} chunks | top similarity: {top_sim:.4f}")

            print(f"\n  [Step {step}B] {label} ‚Äî Sending to LLM for answer...")
            gen = generate_answer(q_text, top_chunks)
            print(f"           Answer (preview): {gen['answer'][:120]}...")

            results_by_model[key] = {
                "retrieved_chunks": top_chunks,
                "llm_answer":       gen["answer"],
                "llm_prompt":       gen["prompt"],
            }

        print(f"\n  ‚úÖ ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà {q_num} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")

        all_results.append({
            "question_number":  q_num,
            "question_text":    q_text,
            "golden_answer":    q.get("golden_answer", ""),
            "results_by_model": results_by_model,
        })

    return all_results
